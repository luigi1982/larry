%%% Preliminaries

%Transformers

%Tutorial
@misc{turnerIntroductionTransformers2024,
  title = {An {{Introduction}} to {{Transformers}}},
  author = {Turner, Richard E.},
  year = 2024,
  month = feb,
  number = {arXiv:2304.10557},
  eprint = {2304.10557},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.10557},
  urldate = {2025-09-02},
  abstract = {The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\phase\\Zotero\\storage\\K5BDAABU\\Turner - 2024 - An Introduction to Transformers.pdf;C\:\\Users\\phase\\Zotero\\storage\\KS9ECND5\\2304.html}
}

%ViT
@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = 2021,
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-10-04},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\phase\\Zotero\\storage\\8LNGYBJK\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;C\:\\Users\\phase\\Zotero\\storage\\PTSN67T8\\2010.html}
}

%SWin Transformer
@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = 2021,
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14030},
  urldate = {2025-08-17},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\phase\\Zotero\\storage\\8IH4NS8B\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf;C\:\\Users\\phase\\Zotero\\storage\\5QV8MG9E\\2103.html}
}

%OCA
@misc{chenHATHybridAttention2024,
  title = {{{HAT}}: {{Hybrid Attention Transformer}} for {{Image Restoration}}},
  shorttitle = {{{HAT}}},
  author = {Chen, Xiangyu and Wang, Xintao and Zhang, Wenlong and Kong, Xiangtao and Qiao, Yu and Zhou, Jiantao and Dong, Chao},
  year = 2024,
  month = nov,
  number = {arXiv:2309.05239},
  eprint = {2309.05239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.05239},
  urldate = {2025-10-04},
  abstract = {Transformer-based methods have shown impressive performance in image restoration tasks, such as image super-resolution and denoising. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better restoration, we propose a new Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to further exploit the potential of the model for further improvement. Extensive experiments have demonstrated the effectiveness of the proposed modules. We further scale up the model to show that the performance of the SR task can be greatly improved. Besides, we extend HAT to more image restoration applications, including real-world image super-resolution, Gaussian image denoising and image compression artifacts reduction. Experiments on benchmark and real-world datasets demonstrate that our HAT achieves state-of-the-art performance both quantitatively and qualitatively. Codes and models are publicly available at https://github.com/XPixelGroup/HAT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\phase\\Zotero\\storage\\AHWFVNP4\\Chen et al. - 2024 - HAT Hybrid Attention Transformer for Image Restoration.pdf;C\:\\Users\\phase\\Zotero\\storage\\QJM9MLLN\\2309.html}
}


%%% SISR Methods

%DRCAN
@misc{zhangImageSuperResolutionUsing2018,
  title = {Image {{Super-Resolution Using Very Deep Residual Channel Attention Networks}}},
  author = {Zhang, Yulun and Li, Kunpeng and Li, Kai and Wang, Lichen and Zhong, Bineng and Fu, Yun},
  year = 2018,
  month = jul,
  number = {arXiv:1807.02758},
  eprint = {1807.02758},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.02758},
  urldate = {2025-10-05},
  abstract = {Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\phase\\Zotero\\storage\\EYHFTMH4\\Zhang et al. - 2018 - Image Super-Resolution Using Very Deep Residual Channel Attention Networks.pdf;C\:\\Users\\phase\\Zotero\\storage\\LMJHKCJR\\1807.html}
}

%SWinIR
@misc{liangSwinIRImageRestoration2021a,
  title = {{{SwinIR}}: {{Image Restoration Using Swin Transformer}}},
  shorttitle = {{{SwinIR}}},
  author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Gool, Luc Van and Timofte, Radu},
  year = 2021,
  month = aug,
  number = {arXiv:2108.10257},
  eprint = {2108.10257},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.10257},
  urldate = {2025-10-06},
  abstract = {Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by \${\textbackslash}textbf\{up to 0.14\${\textbackslash}sim\$0.45dB\}\$, while the total number of parameters can be reduced by \${\textbackslash}textbf\{up to 67\%\}\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\phase\\Zotero\\storage\\4XZZLG5L\\Liang et al. - 2021 - SwinIR Image Restoration Using Swin Transformer.pdf;C\:\\Users\\phase\\Zotero\\storage\\XYMV5TZM\\2108.html}
}


